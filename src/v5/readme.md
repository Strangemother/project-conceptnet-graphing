
## First

Accept sentence input, assess and store the breakdown for submission to the
second layer.

## Second

Analyse a group of words (sentence) to structure an object set of relations,
combining data from the graph database and other externals.

Produce a lager group of selected _contexts_ leveraging weights and mapping
to create an entity for the third layer to study...


## ..+ Third

Given a rich dataset of the sentence to contextualise, utilise the data graph
content to result actions, events and responses - as per context-api reasoning.

Essentially _persist_ sessions for long term usage and graph leveraging. The
information is stored as a complex data graphs or some other _burnt-in_ ml format.

