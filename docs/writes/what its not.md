I had a light discussion with a colleague enjoying my chat on the next ML graph I'm applying and was slightly surprised at the accuracy of the responses. "Where is the cat", "what is the time". "How many legs does a chicken have after dinner" (amusingly 3?). And they begged the answer to _why_ this isn't consciousness as it mimics the answers of their child pretty accurately.

With the data trained upon human information scraped from wikipedia its easy to see why this a tool such as this may be _alive_?

No. It's just clever indexing of data, coupled with some ML voodoo and a lot of hours carefully digesting the right sources. But at times I feel it helps greatly to copy good ideas from the human body - or at least replicate them as best I can.

This in turn starts to push more terminology through a neurological lens. At which point I find myself considering how a machine _thinks_. But of course it doesn't.

I'm wording hard to ensure I don't use vapid cliches to mimic the brain. When communicating about the project, I deem it necessary to communicate precisely and clearly. Due to the nature of the content it's hard to keep a clean view on where the conversation applies. _Is this guy talking about the words in the code, or are these words of a sentence_ right?

So I try hard not to talk about any consciousness; or mimicking aspects of the human brain but sometimes I tend to trip over the best method to explain the code.

In this case "Current context" is a muddy sandbox of in memory references kept alive by recursive weight training and biases to the environment. It sounds technical but it's fairly easy code to write. To explain it succinctly I need to be forgiven for stating _the aware part_ or _live evaluation of current context_. The 'Current Context' is my best way to describe the core _never off_ round robin graphing... \*cough... consciousness.

And with that I'll state - it's nonsense init. I don't believe we'll see a truly conscious computer for at least 50 years. Singularities don't count. I mean a terminator style _"I just woke up, I want to paint a picture"_ true artificial intelligence. So I want to write the 'Brain' and me wee 'neural regions' but that would be silly.

---

Studying a graph of 'alive' - it's pretty narrow, A 'relatedto' 'thing' returns a multitude of results 'hasprerequisite' of a bunch of crap. The system simply stops graphing as it cannot action the edge type naturally.

In these cases I feel canned responses for such question - but this is silly as the graphing should be able to determine 'alive' as some sort of consciousness. of which a 'machine' (in a self referencing graph) is not.

I haven't put much time into researching these graphs so they're quite sparse. More research is needed.
